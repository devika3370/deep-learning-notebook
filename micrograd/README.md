# MicroGrad  

MicroGrad is a minimalistic autograd engine designed to illustrate the core principles of backpropagation. Originally developed by [Andrej Karpathy](https://karpathy.ai/), it serves as an educational tool to understand automatic differentiation from the ground up.  

ðŸ”— **Original Implementation**: [karpathy/micrograd](https://github.com/karpathy/micrograd)  

## Important Links  

- [MicroGrad Explained - YouTube](https://www.youtube.com/watch?v=VMj-3S1tku0)  
- [MicroGrad GitHub Repository](https://github.com/karpathy/micrograd)  
- [Andrej Karpathyâ€™s Website](https://karpathy.ai/)  

## Understanding MicroGrad  

- MicroGrad is a tiny autograd engine that implements **backpropagation**, a key algorithm for computing gradients. 
- Unlike traditional deep learning frameworks that perform parallelized tensor operations, MicroGrad operates at a **scalar level**, offering a low-level, intuitive perspective on how gradients propagate through a computational graph.  
- Computes derivatives efficiently using the chain rule. Represents computations as a directed acyclic graph (DAG).  Graph is plotted using graphviz.
- Propagates gradients through the graph to update model parameters.  Explains how to calculate gradients manually while building the Value() class. 
- Understanding Derivatives: A **derivative** represents the **slope** of a function at a given point. It tells us how much a functionâ€™s output changes with a small input change. MicroGrad builds this understanding by computing derivatives through an explicit computational graph.

- Neuron Structure: 
https://cs231n.github.io/convolutional-networks/

    <img src="images/neuron.png" alt="Neuron" width="400"/>   

- Topological sort is a linear ordering of its vertices such that for every directed edge (u, v) from vertex u to vertex v, u comes before v in ordering

    https://www.geeksforgeeks.org/topological-sorting/

    https://www.interviewcake.com/concept/java/topological-sort